<html>
<head>
</head>

<body>

<p>
say we are running the bar at a soldout <a href="http://en.wikipedia.org/wiki/Bad_religion">bad religion</a> concert.
</p>

<p>
the bar serves beer, scotch and water and so that we can know how much to order for tomorrow's
gig we record orders over the night... 
</p>

<p>
<table>
<tr><td>drink</td><td>#sales</td></tr> 
<tr><td>beer</td><td>1000</td></tr> 
<tr><td>scotch</td><td>300</td></tr> 
<tr><td>water</td><td>200</td></tr> 
</table>
</p>

<p>
using these numbers we can predict a number of things..
</p>

<p>
what is the chance the next person will order a beer? </br>
it's a pretty simple probability; 1000 beers / 1500 total drinks = 0.66 or 66%
</p>

<p>
what is the chance the next person will order a water? </br>
it's a pretty simple probability; 200 waters / 1500 total drinks = 0.14 or 14%
</p>

<p>
now say we run the t-shirt stand at the same concert....
</p>

<p>
instead of only selling 3 items (like at the bar) we sell 20 different types of t-shirts.
again we record orders over the night...
</p>

<p>
<table>
<tr><td>br tour</td><td>15</td><td>pennywise</td><td>3</td><tr>
<tr><td>br logo1</td><td>15</td><td>strung out</td><td>3</td><tr>
<tr><td>br album art 1</td><td>10</td><td>propagandhi</td><td>3</td><tr>
<tr><td>br album art 3</td><td>10</td><td>bouncing souls</td><td>1</td><tr>
<tr><td>nofx logo1</td><td>5</td><td>the vandals</td><td>1</td><tr>
<tr><td>nofx logo2</td><td>5</td><td>dead kennedys</td><td>1</td><tr>
<tr><td>lagwagon</td><td>4</td><td>misfits</td><td>1</td><tr>
<tr><td>frenzal rhomb</td><td>4</td><td>the offspring</td><td>0</td><tr>
<tr><td>rancid</td><td>4</td><td>the ramones</td><td>0</td><tr>
<tr><td>descendants</td><td>3</td><td>mxpx</td><td>0</td><tr>
</table>
</p>

<p>
we can ask similiar questions again regarding the chance of people buying a particular t-shirt
</p>

<p>
what's the chance the next person to buy a t-shirt wants the official tour shirt? 
15 tour shirts sold / 88 sold in total = 0.170 or 17.0%
</p>

<p>
what's the chance the next person to buy a t-shirt wants the a descendants t-shirt?
3 descendants shirts sold / 88 sold in total = 0.034 or 3.4%
</p>

<p>
what's the chance the next person to buy a t-shirt wants the an offspring t-shirt?
0 offspring shirts sold / 88 sold in total = 0 or 0%
</p>

<p>
if you're like me then the last one "feels" wrong, even though we've not seen a purchase of at least 1 shirt
it seems a bit rough to say there is <em>no</em> chance of someone buying one. 
this illustrates one of the problems of dealing 
<a href="http://en.wikipedia.org/wiki/Prior_probability">prior probabilities</a>
</p>

<p>
<pre>
for reference here are the probabilities per shirt
R> sales = rep(c(15,10,5,4,3,1,0), c(2,2,2,3,4,4,3))
R> simple_probs = sales / sum(sales)
 [1] 0.17045455 0.17045455 0.11363636 0.11363636 0.05681818 0.05681818
 [7] 0.04545455 0.04545455 0.04545455 0.03409091 0.03409091 0.03409091
[13] 0.03409091 0.01136364 0.01136364 0.01136364 0.01136364 0.00000000
[19] 0.00000000 0.00000000
</pre>
</p>

<p>
i've discussed the problems with zero probabilities a few times in previous experiments
(<a href="../rss.feed/p3/">here</a> and <a href="../semi_supervised_naive_bayes/semi_supervised_bayes.html">here</a>)
and the approach i've always used is the simple 
<a href="http://en.wikipedia.org/wiki/Rule_of_succession">rule of succession</a> where we avoid
the zero problem by adding one to each probability
</p>

<p>
<pre>
here are the values for the succession rule case

R> sales_plus_one = x+1
R> smooth_probs = sales_plus_one / sum(sales_plus_one)
 [1] 0.14814815 0.14814815 0.10185185 0.10185185 0.05555556 0.05555556
 [7] 0.04629630 0.04629630 0.04629630 0.03703704 0.03703704 0.03703704
[13] 0.03703704 0.01851852 0.01851852 0.01851852 0.01851852 0.00925926
[19] 0.00925926 0.00925926
</pre>
</p>

<p>
it's always kind of worked (well at least better than having the zeros) but it's always felt wrong too. 
</p>

<p>
finally the other day i found another approach, that seems a lot more statistically sound.
</p>

<p>
it's called <a href="http://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation">good-turing estimation</a> 
and it was developed as part of <a href="http://en.wikipedia.org/wiki/Alan_Turing">turing's</a> work at bletchley 
park (so it must be awesome). a decent implementation is explained in 
<a href="http://www.grsampson.net/AGtf1.html">this paper</a> by geoffrey sampson (it's somewhat more 
complex than adding 1)
</p>

<p>
it works on using the frequency of frequencies and redistributes the probabilities to include a
0th entry which is total allocation we should give to items never seen before.
</p>

<p>
in our case the total allocation for the 0th case is 0.045, since we had 3 cases of shirts that weren't
seen we distribute this 0.045 across the 3.
</p>

<p>
<table>
15 2 0.170 0.148 0.160
10 2 0.113 0.101 0.107
5  2 0.056 0.055 0.054
4  3 0.045 0.046 0.043
3  4 0.034 0.037 0.033
1  4 0.011 0.018 0.011
0  - 0.000 0.009 0.015
</table>
</p>

<img src="tshirts.png"/>

<p>
what i think is the most interesting is that it puts the t-shirts that <em>weren't</em> brought
at an estimated purchase frequency higher than the ones that actually <em>were</em> purchased once.
</p>

<p>
now this is all good, but i actually don't run the bar at a bad religion concert (or the t-shirt stand) 
and i'm actually interested in this in how it applies to text processing...
</p>

<p>
so how might we measure if good-turing is a better estimate than the much simpler succession approach?
</p>

<p>
consider a <a href="tweets.txt.gz">small sample of 15e3 or so tweets</a>, 
that i've run through a simple <a href="http://www.nltk.org/">ntlk</a> tokeniser which retains 
#hashtags and @user_mentions and only emits only the domain of urls (after derferencing 
url shorteners if required) to generate a <a href="tokenised.txt.gz">tokenised version</a>
</p>

<pre>
eg
RT @jeffbarr: New AWS EC2 Instance Type: Cluster GPU - http://bit.ly/ec2gpu  #hpc #nvidia #wow
becomes
rt @jeffbarr new aws ec2 instance type cluster gpu [aws.typepad.com] #hpc #nvidia #wow
</pre>

<p>
some stats about the tweets include: 15e3 tweets, 218e3 tokens total, 27e3 terms
</p>

<p>
tokens follow a classic zipf power law distribution with top tokens being what we expect...
<pre>
freqs = { 'the' => 6348, 'to' => 4921, 'a' => 4270, 'i' => 3475, 'of' => 3159, 'in' => 2757, 'is' => 2629, ... }
</pre>

<p>
with a tail end of 15e3 terms (of the 27e3 total) occuring only once in the corpus.
</p>

<p>
let's test our estimators by seeing how well the first half of the tweets can predict the frequency in the second half
</p>





### wip stuff

./list_tweets.rb > tweets.txt
./emit_tokens.rb > tokenised.txt
./emit_tokens_one_per_line.rb > tokens.txt

sort tokens.txt | uniq -c | sort -nr > tokens.freq.txt
cat tokens.freq.txt | perl -plne's/^\s+(\d+).*/$1/' > freqs
cat freqs | uniq -c | sort -nr > freqs_of_freqs






